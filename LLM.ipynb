{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3422fcf3-a428-48af-9e21-513e8496ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc11de9c-b10d-41f9-a343-21b7e5603595",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text=\"\"\" \n",
    "the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres\n",
    "\n",
    "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ad007a-2f14-49eb-baf1-6fb32bc6d1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\VICTUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faad08a911364e139c68c5c3a19489ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTUS\\anaconda36\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VICTUS\\.cache\\huggingface\\hub\\models--cnicu--t5-small-booksum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514c7447f56d41799e7fb86ee7cd8291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5febac2864df4b4981d978f6498072ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a95feaafe1488b8df856286a1bbf27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e40e737cec64af7a21e9e1d7addcf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bb4f459afe44cb9347c553c4f614fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "summarizer= pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e350c53a-669b-4ac6-b5fe-19ff21a4790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text:\n",
      "the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris . Its base is square, measuring 125 metres (410 mt) on each side. It is the second tallest free-standing structure in France after the Millau Viaduct.\n"
     ]
    }
   ],
   "source": [
    "output = summarizer(long_text, max_length=50, min_length=25, do_sample=False)\n",
    "\n",
    "# Access and print the summarized text\n",
    "print(\"Summarized Text:\")\n",
    "print(output[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4070a71c-142f-4f7a-a1d3-9d18a3d2b88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cacce7d80b4ae687a3e9596637a10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTUS\\anaconda36\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VICTUS\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce9fbf1fa004a1ea1b00dda534ad0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6a588e0e8645979e14fa2b28bf3746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02835bcb89ff499caafca914e3fc2b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5bb81dca064f35990ec36cc91bc043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd281b0a9ff41b4aa3c8e1820dfb9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d575126520474218a93e43555c3b3e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "## Text Generation \n",
    "\n",
    "generator=pipeline(task=\"text-generation\",model=\"gpt2\")\n",
    "\n",
    "customer_review=\"\"\"\n",
    "Booked today by Anonymous\n",
    "I had a wonderful stay at the Riverview Hotel! \n",
    "The staff were incredibly attentive and the amenities were top-notch.\n",
    "The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7068f70-25d3-4c9b-9b38-7965aa55b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb8daadc-5a99-4073-b26b-e0fa7562b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=f\"Customer Review:\\n{customer_review}\\n\\nHotel Response to customer:\\n{response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fabec53-59e1-4d18-9235-082858a7fcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "outputs=generator(prompt,max_length=150,pad_token_id=generator.tokenizer.eos_token_id,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55dd3df8-91cc-43b3-85f4-566cc240487e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output by model: Customer Review:\n",
      "\n",
      "Booked today by Anonymous\n",
      "I had a wonderful stay at the Riverview Hotel! \n",
      "The staff were incredibly attentive and the amenities were top-notch.\n",
      "The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\n",
      "\n",
      "\n",
      "Hotel Response to customer:\n",
      "Dear valued customer, I am glad to hear you had a good stay with us. We are very pleased with the quality and service we have received from you.\n",
      "\n",
      "\n",
      "Thank you.\n",
      "\n",
      "\n",
      "The quality of our rooms, the accommodation, and the customer service are top notch.\n",
      "\n",
      "\n",
      "Booked in the U.S. & Canada, and booked in the U.K. with the cheapest rates in the world. Great memories in my life.\n",
      "\n",
      "\n",
      "The service and amenities were great. The wait staff was super friendly and helpful.\n",
      "\n",
      "I am going to be staying here for a long time and I will be returning. Would recommend to anyone wanting a great stay at a great hotel.\n",
      "\n",
      "\n",
      "Booked in the U.S. and Canada, and booked in the U.K. with the cheapest rates in the world. Great memories in my life.\n",
      "\n",
      "\n",
      "Best Hotel in the Western Hemisphere. I've been staying in the U.S. for over 5 years. I have been to many places and can't say I've ever been happier with this hotel.\n",
      "\n",
      "\n",
      "My visit to Riverview Hotel is a must-visit for all those who enjoy the river. The view, the views, the sounds and everything about this place is breathtaking. I am so thankful to be staying in this beautiful hotel and it is a must\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated output by model:\",outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72233457-4395-4cce-aaa0-83030a41cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transalation from Spanish to English \n",
    "\n",
    "spanish_text = \"Este curso sobre LLMs se estÃ¡ poniendo muy interesante\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858e7c9d-6176-4d02-aff0-07d7fc1fb399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\VICTUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40696caa499a439786f4e221e6f8b5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/826k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTUS\\anaconda36\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VICTUS\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-es-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e005855284c5423986cda47d7e7bc806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc84ab5edad4c91b561b423960df3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTUS\\anaconda36\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "translator=pipeline(task=\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6402ed52-33aa-437a-b698-4a9f8c3d4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations=translator(spanish_text,clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8cf4f72-d130-4c7a-a6c5-85c920af1c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This course on LLMs is getting very interesting.\n"
     ]
    }
   ],
   "source": [
    "print(translations[0][\"translation_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c83db433-3783-4200-b378-f1ab9898d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarianMTModel(\n",
      "  (model): MarianModel(\n",
      "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
      "    (encoder): MarianEncoder(\n",
      "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
      "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): SiLU()\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): MarianDecoder(\n",
      "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
      "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (activation_fn): SiLU()\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(translator.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "307d972a-dcf3-4855-a4f2-4023e99f088b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0a28fab66845d5b6d2beffb1ec5869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTUS\\anaconda36\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VICTUS\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21872e84f49467b93b7536437ca28f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd5463880c44bddb379dab709382bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e94a539f95470caeb09e8e84de01a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abf54c3c5b34ca898b3f2aff8d0a619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "qa=pipeline(task=\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06cda39d-e9c2-4894-a12b-8d3fb5ae1e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leonardo da Vinci\n"
     ]
    }
   ],
   "source": [
    "question=\"Who painted the Mona Lisa?\"\n",
    "context=\"\"\"\n",
    "The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci. Considered an archetypal masterpiece of the Italian Renaissance, it has been described as the most known, visited, talked about, and sung about work of art in the world. The painting's novel qualities include the subject's enigmatic expression, the monumentality of the composition, and the subtle modeling of forms.\n",
    "\"\"\"\n",
    "output=qa(question=question,context=context)\n",
    "\n",
    "print(output['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4079f-67ab-4b55-8770-9a56538e3f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
