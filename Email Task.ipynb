{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67169d68-a8b9-49c5-ba0e-9644c14ef1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of our email dataset:\n",
      "   email_id                                      email_content  \\\n",
      "0         1  Urgent: Server Maintenance Required\\nOur main ...   \n",
      "1         2  50% Off Spring Collection!\\nDon't miss our big...   \n",
      "\n",
      "  expected_category  \n",
      "0          Priority  \n",
      "1        Promotions  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e147a2ee22764e6e85001231e7e1087b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTUS\\anaconda36\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VICTUS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bf58584e73448fa28070e53b180a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb4aa240894413dbf32ddd87efe1ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30591ac69aea46c7acc47d541c05cdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e976f79e824adab46fa519695e1ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Results:\n",
      "Email 1:\n",
      " - Review: Urgent: Server Maintenance Required\\nOur main server needs immediate maintenance due to critical errors. Please address ASAP.\n",
      " - Expected Category: Priority\n",
      " - Predicted Category: Updates\n",
      "\n",
      "Email 2:\n",
      " - Review: 50% Off Spring Collection!\\nDon't miss our biggest sale of the season! All spring items half off. Limited time offer.\n",
      " - Expected Category: Promotions\n",
      " - Predicted Category: Promotions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your_token' with the token you just generated\n",
    "login(token=\"your_token\")\n",
    "\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the email dataset\n",
    "emails_df = pd.read_csv('data/email_categories_data.csv')\n",
    "\n",
    "# Display the first few rows of our dataset\n",
    "print(\"Preview of our email dataset:\")\n",
    "print(emails_df.head(2))\n",
    "\n",
    "# Define the model name (Llama model from Hugging Face)\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Initialize the Hugging Face pipeline for text classification using Llama\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "\n",
    "# Create the system prompt with examples (you can customize this part based on your task)\n",
    "prompt = \"\"\" You classify emails into Priority, Updates, or Promotions.\n",
    "\n",
    "Example 1:\n",
    "Urgent: Password Reset Required\n",
    "Your account security requires immediate attention. Please reset your password within 24 hours.\n",
    "Response: Priority\n",
    "\n",
    "Example 2:\n",
    "Special Offer - 50% Off Everything!\n",
    "Don't miss our biggest sale of the year. Everything must go!\n",
    "Response: Promotions\n",
    "\n",
    "Example 3:\n",
    "Canceled Event - Team Meeting\n",
    "This event has been canceled and removed from your calendar.\n",
    "Response: Updates\n",
    "\n",
    "Example 4:\n",
    "\"\"\"\n",
    "\n",
    "# Function to process messages and return classifications\n",
    "def process_message(classifier, message, prompt):\n",
    "    \"\"\"Process a message and return the response\"\"\"\n",
    "    input_prompt = f\"{prompt} {message}\"\n",
    "    \n",
    "    # Perform classification using the classifier\n",
    "    result = classifier(input_prompt, candidate_labels=[\"Priority\", \"Updates\", \"Promotions\"])\n",
    "    \n",
    "    # Extract the label with the highest score\n",
    "    return result['labels'][0]  # This gives the highest scoring label\n",
    "\n",
    "# Let's test our classifier on two emails from our dataset\n",
    "test_emails = emails_df.head(2)\n",
    "\n",
    "# Process each test email and store results\n",
    "results = []\n",
    "for idx, row in test_emails.iterrows():\n",
    "    email_content = row['email_content']\n",
    "    expected_category = row['expected_category']\n",
    "    \n",
    "    # Get model's classification\n",
    "    result = process_message(classifier, email_content, prompt)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'email_content': email_content,\n",
    "        'expected_category': expected_category,\n",
    "        'model_output': result\n",
    "    })\n",
    "\n",
    "# Create a DataFrame with results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nClassification Results:\")\n",
    "for index, result in results_df.iterrows():\n",
    "    print(f\"Email {index + 1}:\")\n",
    "    print(f\" - Review: {result['email_content']}\")\n",
    "    print(f\" - Expected Category: {result['expected_category']}\")\n",
    "    print(f\" - Predicted Category: {result['model_output']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca5bf7ab-3d86-4700-8b84-1cc7c4f8901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b1248-a556-4e1f-a39e-56ea5d25223b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
